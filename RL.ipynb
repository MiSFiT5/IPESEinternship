{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共享层和专用层分别处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# 假设已有的 name_count, A, B 数据\n",
    "name_count = {\n",
    "    'var1': 1, 'var2': 2, 'var3': 3,  # 示例数据，实际应包含所有73个配置\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# 定义 A 和 B\n",
    "A = np.random.uniform(-1, 1, size=(3, 73))  # 假设 A 的真实数据\n",
    "B = np.random.uniform(-1, 1, size=(3,))    # 假设 B 的真实数据\n",
    "\n",
    "# 解析配置变量\n",
    "variables = list(name_count.keys())\n",
    "participation_count = list(name_count.values())\n",
    "\n",
    "# 找出共享变量的索引\n",
    "shared_indices = [i for i, count in enumerate(participation_count) if count > 1]\n",
    "shared_size = len(shared_indices)\n",
    "\n",
    "# 自定义环境类，包含 KPI 计算逻辑\n",
    "class FactoryEnv(gym.Env):\n",
    "    def __init__(self, state_size, action_size, A, B):\n",
    "        super(FactoryEnv, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(state_size,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size,), dtype=np.float32)\n",
    "        self.state = np.random.uniform(-1, 1, size=(state_size,))\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state = np.clip(self.state + action, -1, 1)\n",
    "        kpi1 = np.dot(self.A[0], self.state) + self.B[0]\n",
    "        kpi2 = np.dot(self.A[1], self.state) + self.B[1]\n",
    "        kpi3 = np.dot(self.A[2], self.state) + self.B[2]\n",
    "        reward_vector = np.array([kpi1, kpi2, kpi3])  # 多目标奖励\n",
    "        done = False  # 根据需要设置终止条件\n",
    "        return self.state, reward_vector, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-1, 1, size=(self.state_size,))\n",
    "        return self.state\n",
    "\n",
    "# 定义 Actor 和 Critic 网络\n",
    "class SharedActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        super(SharedActor, self).__init__()\n",
    "        self.shared_fc = nn.Linear(shared_size, 128)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, 128)\n",
    "        self.fc1 = nn.Linear(256 + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class SharedCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        super(SharedCritic, self).__init__()\n",
    "        self.shared_fc = nn.Linear(shared_size, 128)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, 128)\n",
    "        self.fc1 = nn.Linear(256 + action_size + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state, action, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, action, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义 MORL 智能体\n",
    "class MORLAgent:\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.weight_size = weight_size\n",
    "        self.actor = SharedActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.critic = SharedCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_actor = SharedActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_critic = SharedCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        # 初始化目标网络参数\n",
    "        self._update_target(self.target_actor, self.actor, 1.0)\n",
    "        self._update_target(self.target_critic, self.critic, 1.0)\n",
    "\n",
    "    def _update_target(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, weights):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state, weights).squeeze(0).numpy()\n",
    "        return np.clip(action + np.random.normal(0, 0.1, size=self.action_size), -1, 1)\n",
    "\n",
    "    def remember(self, state, action, reward_vector, next_state, done, weights):\n",
    "        weighted_reward = np.dot(weights, reward_vector)\n",
    "        self.memory.append((state, action, weighted_reward, next_state, done, weights))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, weights = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        # Critic 更新\n",
    "        next_actions = self.target_actor(next_states, weights)\n",
    "        next_q_values = self.target_critic(next_states, next_actions, weights)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        q_values = self.critic(states, actions, weights)\n",
    "        critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor 更新\n",
    "        actions_pred = self.actor(states, weights)\n",
    "        actor_loss = -self.critic(states, actions_pred, weights).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        self._update_target(self.target_actor, self.actor, self.tau)\n",
    "        self._update_target(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = FactoryEnv(state_size=73, action_size=73, A=A, B=B)\n",
    "agent = MORLAgent(state_size=73, action_size=73, shared_size=len(shared_indices), weight_size=3)\n",
    "\n",
    "# 开始训练\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    weights = np.random.dirichlet(np.ones(3), size=1)[0]\n",
    "    while not done:\n",
    "        action = agent.act(state, weights)\n",
    "        next_state, reward_vector, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward_vector, next_state, done, weights)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        episode_reward += np.dot(weights, reward_vector)\n",
    "    print(f\"Episode {episode + 1}, Weighted Reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 name_count_dict 已定义\n",
    "name_count_dict = name_count_dict  # 示例变量\n",
    "# 定义 A 和 B\n",
    "A = np.random.uniform(-1, 1, size=(3, 73))  # 假设 A 的真实数据\n",
    "B = np.random.uniform(-1, 1, size=(3,))    # 假设 B 的真实数据\n",
    "\n",
    "# 解析配置变量\n",
    "variables = list(name_count_dict.keys())\n",
    "participation_count = list(name_count_dict.values())\n",
    "\n",
    "# 找出共享变量的索引\n",
    "shared_indices = [i for i, count in enumerate(participation_count) if count > 1]\n",
    "shared_size = len(shared_indices)\n",
    "\n",
    "# 自定义环境类，包含 KPI 计算逻辑\n",
    "class FactoryEnv(gym.Env):\n",
    "    def __init__(self, state_size, action_size, A, B):\n",
    "        super(FactoryEnv, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(state_size,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size,), dtype=np.float32)\n",
    "        self.state = np.random.uniform(-1, 1, size=(state_size,))\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state = np.clip(self.state + action, -1, 1)\n",
    "        kpi1 = np.dot(self.A[0], self.state) + self.B[0]\n",
    "        kpi2 = np.dot(self.A[1], self.state) + self.B[1]\n",
    "        kpi3 = np.dot(self.A[2], self.state) + self.B[2]\n",
    "        reward_vector = np.array([kpi1, kpi2, kpi3])  # 多目标奖励\n",
    "        done = False  # 根据需要设置终止条件\n",
    "        return self.state, reward_vector, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-1, 1, size=(self.state_size,))\n",
    "        return self.state\n",
    "\n",
    "# 定义 Actor 和 Critic 网络\n",
    "class SharedActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        super(SharedActor, self).__init__()\n",
    "        self.shared_fc = nn.Linear(shared_size, 128)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, 128)\n",
    "        self.fc1 = nn.Linear(256 + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class SharedCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        super(SharedCritic, self).__init__()\n",
    "        self.shared_fc = nn.Linear(shared_size, 128)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, 128)\n",
    "        self.fc1 = nn.Linear(256 + action_size + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state, action, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, action, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义 MORL 智能体\n",
    "class MORLAgent:\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.weight_size = weight_size\n",
    "        self.actor = SharedActor(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.critic = SharedCritic(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.target_actor = SharedActor(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.target_critic = SharedCritic(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        # 初始化目标网络参数\n",
    "        self._update_target(self.target_actor, self.actor, 1.0)\n",
    "        self._update_target(self.target_critic, self.critic, 1.0)\n",
    "\n",
    "    def _update_target(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, weights):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state, weights).squeeze(0).cpu().numpy()\n",
    "        return np.clip(action + np.random.normal(0, 0.1, size=self.action_size), -1, 1)\n",
    "\n",
    "    def remember(self, state, action, reward_vector, next_state, done, weights):\n",
    "        weighted_reward = np.dot(weights, reward_vector)\n",
    "        self.memory.append((state, action, weighted_reward, next_state, done, weights))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, weights = zip(*batch)\n",
    "        \n",
    "        # 将 numpy 数组列表合并为单个 numpy 数组来提高效率\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.FloatTensor(np.array(actions)).to(device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(device)\n",
    "        weights = torch.FloatTensor(np.array(weights)).to(device)\n",
    "        \n",
    "        # Critic 更新\n",
    "        next_actions = self.target_actor(next_states, weights)\n",
    "        next_q_values = self.target_critic(next_states, next_actions, weights)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        q_values = self.critic(states, actions, weights)\n",
    "        critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor 更新\n",
    "        actions_pred = self.actor(states, weights)\n",
    "        actor_loss = -self.critic(states, actions_pred, weights).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        self._update_target(self.target_actor, self.actor, self.tau)\n",
    "        self._update_target(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "# 设备设置：使用 GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = FactoryEnv(state_size=73, action_size=73, A=A, B=B)\n",
    "agent = MORLAgent(state_size=73, action_size=73, shared_size=len(shared_indices), weight_size=3)\n",
    "\n",
    "# 记录训练数据\n",
    "reward_history = []\n",
    "critic_loss_history = []\n",
    "\n",
    "# 开始训练\n",
    "episodes = 100\n",
    "max_steps = 2000  # 设置最大步数来控制 episode 长度\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    weights = np.random.dirichlet(np.ones(3), size=1)[0]\n",
    "    step_count = 0\n",
    "    episode_critic_loss = 0\n",
    "    while not done and step_count < max_steps:\n",
    "        action = agent.act(state, weights)\n",
    "        next_state, reward_vector, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward_vector, next_state, done, weights)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        episode_reward += np.dot(weights, reward_vector)\n",
    "        step_count += 1\n",
    "        \n",
    "        # 记录 Critic 损失\n",
    "        q_values = agent.critic(torch.FloatTensor(state).unsqueeze(0).to(device),\n",
    "                                torch.FloatTensor(action).unsqueeze(0).to(device),\n",
    "                                torch.FloatTensor(weights).unsqueeze(0).to(device))\n",
    "        target_q_values = torch.FloatTensor([episode_reward]).unsqueeze(1).to(device)\n",
    "        critic_loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        episode_critic_loss += critic_loss.item()\n",
    "        \n",
    "    reward_history.append(episode_reward)\n",
    "    critic_loss_history.append(episode_critic_loss / step_count)  # 平均损失\n",
    "    print(f\"Episode {episode + 1}, Weighted Reward: {episode_reward}, Critic Loss: {episode_critic_loss / step_count}\")\n",
    "\n",
    "# 绘制训练过程中的加权奖励和 Critic 损失图像\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(reward_history)\n",
    "plt.title('Weighted Reward Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Weighted Reward')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(critic_loss_history)\n",
    "plt.title('Critic Loss Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Critic Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# 自定义多头注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)  # 增加批次维度\n",
    "        attn_output, _ = self.attention(x, x, x)  # 自注意力\n",
    "        return attn_output.squeeze(0)  # 移除批次维度\n",
    "\n",
    "# Actor 网络，带多头注意力机制\n",
    "class AttentionActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=128, num_heads=4):\n",
    "        super(AttentionActor, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state, weights):\n",
    "        shared_features = state[:, shared_indices]\n",
    "        shared_features = self.shared_attention(shared_features)\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "# Critic 网络，带多头注意力机制\n",
    "class AttentionCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=128, num_heads=4):\n",
    "        super(AttentionCritic, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + action_size + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state, action, weights):\n",
    "        shared_features = state[:, shared_indices]\n",
    "        shared_features = self.shared_attention(shared_features)\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, action, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义 MORL 智能体\n",
    "class MORLAgent:\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.weight_size = weight_size\n",
    "        self.actor = AttentionActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.critic = AttentionCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_actor = AttentionActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_critic = AttentionCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        # 初始化目标网络参数\n",
    "        self._update_target(self.target_actor, self.actor, 1.0)\n",
    "        self._update_target(self.target_critic, self.critic, 1.0)\n",
    "\n",
    "    def _update_target(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, weights):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state, weights).squeeze(0).numpy()\n",
    "        return np.clip(action + np.random.normal(0, 0.1, size=self.action_size), -1, 1)\n",
    "\n",
    "    def remember(self, state, action, reward_vector, next_state, done, weights):\n",
    "        weighted_reward = np.dot(weights, reward_vector)\n",
    "        self.memory.append((state, action, weighted_reward, next_state, done, weights))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, weights = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        # Critic 更新\n",
    "        next_actions = self.target_actor(next_states, weights)\n",
    "        next_q_values = self.target_critic(next_states, next_actions, weights)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        q_values = self.critic(states, actions, weights)\n",
    "        critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor 更新\n",
    "        actions_pred = self.actor(states, weights)\n",
    "        actor_loss = -self.critic(states, actions_pred, weights).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        self._update_target(self.target_actor, self.actor, self.tau)\n",
    "        self._update_target(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = FactoryEnv(state_size=73, action_size=73, A=A, B=B)\n",
    "agent = MORLAgent(state_size=73, action_size=73, shared_size=len(shared_indices), weight_size=3)\n",
    "\n",
    "# 开始训练\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    weights = np.random.dirichlet(np.ones(3), size=1)[0]\n",
    "    while not done:\n",
    "        action = agent.act(state, weights)\n",
    "        next_state, reward_vector, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward_vector, next_state, done, weights)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        episode_reward += np.dot(weights, reward_vector)\n",
    "    print(f\"Episode {episode + 1}, Weighted Reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "from tqdm import tqdm  # 进度条显示\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "name_count = pd.read_csv('name_count.csv')\n",
    "# 转化成字典\n",
    "name_count_dict = name_count.set_index('Unnamed: 0')['count'].to_dict()\n",
    "\n",
    "variables = list(name_count_dict.keys())\n",
    "participation_count = list(name_count_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "# 找出共享变量的索引（即参与次数超过一次的变量）\n",
    "shared_indices = [i for i, count in enumerate(participation_count) if count > 1]\n",
    "shared_size = len(shared_indices)\n",
    "\n",
    "# 定义 A 和 B\n",
    "A = np.random.uniform(-1, 1, size=(3, 73))  # 假设 A 的真实数据\n",
    "B = np.random.uniform(-1, 1, size=(3,))    # 假设 B 的真实数据\n",
    "\n",
    "# 自定义环境类，包含 KPI 计算逻辑\n",
    "class FactoryEnv(gym.Env):\n",
    "    def __init__(self, state_size, action_size, A, B, max_steps=25000):\n",
    "        super(FactoryEnv, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(state_size,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size,), dtype=np.float32)\n",
    "        self.state = np.random.uniform(-1, 1, size=(state_size,))\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        self.state = np.clip(self.state + action, -1, 1)\n",
    "        kpi1 = np.dot(self.A[0], self.state) + self.B[0]\n",
    "        kpi2 = np.dot(self.A[1], self.state) + self.B[1]\n",
    "        kpi3 = np.dot(self.A[2], self.state) + self.B[2]\n",
    "        reward_vector = np.array([kpi1, kpi2, kpi3])  # 多目标奖励\n",
    "        done = self.current_step >= self.max_steps  # 根据步数终止\n",
    "        return self.state, reward_vector, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-1, 1, size=(self.state_size,))\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "# 自定义多头注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)  # 自注意力\n",
    "        return attn_output\n",
    "\n",
    "# Actor 网络，带多头注意力机制\n",
    "class AttentionActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=63, num_heads=3):  # 简化 embed_dim 和 num_heads\n",
    "        super(AttentionActor, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.shared_fc = nn.Linear(shared_size, embed_dim)  # 使用 shared_size\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state, weights):\n",
    "        # 提取共享特征，使用 shared_indices\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        shared_features = shared_features.unsqueeze(1)  # 调整为 (batch_size, seq_length, embed_dim) 格式\n",
    "        shared_features = self.shared_attention(shared_features).squeeze(1)\n",
    "        \n",
    "        # 提取非共享特征\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "# Critic 网络，带多头注意力机制\n",
    "class AttentionCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=63, num_heads=3):  # 简化 embed_dim 和 num_heads\n",
    "        super(AttentionCritic, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.shared_fc = nn.Linear(shared_size, embed_dim)  # 使用 shared_size\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + action_size + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state, action, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        shared_features = shared_features.unsqueeze(1)  # 调整为 (batch_size, seq_length, embed_dim) 格式\n",
    "        shared_features = self.shared_attention(shared_features).squeeze(1)\n",
    "        \n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, action, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义 MORL 智能体\n",
    "class MORLAgent:\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.weight_size = weight_size\n",
    "        self.device = device\n",
    "        self.actor = AttentionActor(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.critic = AttentionCritic(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.target_actor = AttentionActor(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.target_critic = AttentionCritic(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.learn_every = 10  # 每隔多少步进行一次学习\n",
    "        self.critic_loss_history = []  # 存储 Critic 损失历史\n",
    "        \n",
    "        # 初始化目标网络参数\n",
    "        self._update_target(self.target_actor, self.actor, 1.0)\n",
    "        self._update_target(self.target_critic, self.critic, 1.0)\n",
    "\n",
    "    def _update_target(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, weights):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state, weights).squeeze(0).cpu().numpy()\n",
    "        return np.clip(action + np.random.normal(0, 0.1, size=self.action_size), -1, 1)\n",
    "\n",
    "    def remember(self, state, action, reward_vector, next_state, done, weights):\n",
    "        weighted_reward = np.dot(weights, reward_vector)\n",
    "        self.memory.append((state, action, weighted_reward, next_state, done, weights))\n",
    "\n",
    "    def learn(self, step_count):\n",
    "        if len(self.memory) < self.batch_size or step_count % self.learn_every != 0:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, weights = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.FloatTensor(np.array(actions)).to(self.device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(self.device)\n",
    "        weights = torch.FloatTensor(np.array(weights)).to(self.device)\n",
    "        \n",
    "        # Critic 更新\n",
    "        next_actions = self.target_actor(next_states, weights)\n",
    "        next_q_values = self.target_critic(next_states, next_actions, weights)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        q_values = self.critic(states, actions, weights)\n",
    "        critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # 保存 Critic 损失\n",
    "        self.critic_loss_history.append(critic_loss.item())\n",
    "        \n",
    "        # Actor 更新\n",
    "        actions_pred = self.actor(states, weights)\n",
    "        actor_loss = -self.critic(states, actions_pred, weights).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        self._update_target(self.target_actor, self.actor, self.tau)\n",
    "        self._update_target(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "# 设备设置：使用 GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = FactoryEnv(state_size=73, action_size=73, A=A, B=B, max_steps=25000)\n",
    "agent = MORLAgent(state_size=73, action_size=73, shared_size=len(shared_indices), weight_size=3, device=device)\n",
    "\n",
    "# 记录训练数据\n",
    "reward_history = []\n",
    "critic_loss_history = []\n",
    "\n",
    "# 开始训练\n",
    "episodes = 100\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    weights = np.random.dirichlet(np.ones(3), size=1)[0]\n",
    "    step_count = 0\n",
    "\n",
    "    # 使用 tqdm 显示训练进度\n",
    "    with tqdm(total=env.max_steps, desc=f\"Episode {episode + 1}\") as pbar:\n",
    "        while not done:\n",
    "            action = agent.act(state, weights)\n",
    "            next_state, reward_vector, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward_vector, next_state, done, weights)\n",
    "            agent.learn(step_count)\n",
    "            state = next_state\n",
    "            episode_reward += np.dot(weights, reward_vector)\n",
    "            step_count += 1\n",
    "            pbar.update(1)  # 更新进度条\n",
    "\n",
    "    reward_history.append(episode_reward)\n",
    "    critic_loss_history.append(np.mean(agent.critic_loss_history[-step_count:]))  # 记录每个 episode 的平均 Critic 损失\n",
    "    print(f\"Episode {episode + 1}, Weighted Reward: {episode_reward}, Critic Loss: {critic_loss_history[-1]}\")\n",
    "\n",
    "# 绘制训练过程中的加权奖励和 Critic 损失图像\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(reward_history, label='Weighted Reward')\n",
    "plt.title('Weighted Reward Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Weighted Reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(critic_loss_history, label='Critic Loss')\n",
    "plt.title('Critic Loss Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Critic Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 name_count.csv 转化为字典\n",
    "name_count = pd.read_csv('name_count.csv')\n",
    "name_count_dict = name_count.set_index('Unnamed: 0')['count'].to_dict()\n",
    "\n",
    "variables = list(name_count_dict.keys())\n",
    "participation_count = list(name_count_dict.values())\n",
    "\n",
    "# 找出共享变量的索引（即参与次数超过一次的变量）\n",
    "shared_indices = [i for i, count in enumerate(participation_count) if count > 1]\n",
    "shared_size = len(shared_indices)\n",
    "\n",
    "# 定义 A 和 B 的初始值\n",
    "A_mean = 0  # 假设均值\n",
    "A_std = 1   # 假设标准差\n",
    "B = np.random.uniform(-1, 1, size=(3,))  # B 的固定值\n",
    "\n",
    "# 自定义环境类，包含 KPI 计算逻辑\n",
    "class FactoryEnv(gym.Env):\n",
    "    def __init__(self, state_size, action_size, B, max_steps=50000):\n",
    "        super(FactoryEnv, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.B = B\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.A = np.random.normal(A_mean, A_std, size=(3, state_size))  # 初始的 A\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(state_size,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size,), dtype=np.float32)\n",
    "        self.state = np.random.uniform(-1, 1, size=(state_size,))\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        self.state = np.clip(self.state + action, -1, 1)\n",
    "        kpi1 = np.dot(self.A[0], self.state) + self.B[0]\n",
    "        kpi2 = np.dot(self.A[1], self.state) + self.B[1]\n",
    "        kpi3 = np.dot(self.A[2], self.state) + self.B[2]\n",
    "        reward_vector = np.array([kpi1, kpi2, kpi3])  # 多目标奖励\n",
    "        done = self.current_step >= self.max_steps  # 根据步数终止\n",
    "        return self.state, reward_vector, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-1, 1, size=(self.state_size,))\n",
    "        self.current_step = 0\n",
    "        # 每次重置时重新生成 A 的值\n",
    "        self.A = np.random.normal(A_mean, A_std, size=(3, self.state_size))\n",
    "        return self.state\n",
    "\n",
    "# 自定义多头注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)  # 自注意力\n",
    "        return attn_output\n",
    "\n",
    "# Actor 网络，带多头注意力机制\n",
    "class AttentionActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=63, num_heads=3):\n",
    "        super(AttentionActor, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.shared_fc = nn.Linear(shared_size, embed_dim)  # 使用 shared_size\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        shared_features = shared_features.unsqueeze(1)  # 调整为 (batch_size, seq_length, embed_dim) 格式\n",
    "        shared_features = self.shared_attention(shared_features).squeeze(1)\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "# Critic 网络，带多头注意力机制\n",
    "class AttentionCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=63, num_heads=3):\n",
    "        super(AttentionCritic, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.shared_fc = nn.Linear(shared_size, embed_dim)  # 使用 shared_size\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + action_size + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state, action, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        shared_features = shared_features.unsqueeze(1)  # 调整为 (batch_size, seq_length, embed_dim) 格式\n",
    "        shared_features = self.shared_attention(shared_features).squeeze(1)\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, action, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义 MORL 智能体\n",
    "class MORLAgent:\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.weight_size = weight_size\n",
    "        self.device = device\n",
    "        self.actor = AttentionActor(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.critic = AttentionCritic(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.target_actor = AttentionActor(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.target_critic = AttentionCritic(state_size, action_size, shared_size, weight_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.learn_every = 10  # 每隔多少步进行一次学习\n",
    "        self.critic_loss_history = []  # 存储 Critic 损失历史\n",
    "        \n",
    "        # 初始化目标网络参数\n",
    "        self._update_target(self.target_actor, self.actor, 1.0)\n",
    "        self._update_target(self.target_critic, self.critic, 1.0)\n",
    "\n",
    "    def _update_target(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, weights):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state, weights).squeeze(0).cpu().numpy()\n",
    "        return np.clip(action + np.random.normal(0, 0.1, size=self.action_size), -1, 1)\n",
    "\n",
    "    def remember(self, state, action, reward_vector, next_state, done, weights):\n",
    "        weighted_reward = np.dot(weights, reward_vector)\n",
    "        self.memory.append((state, action, weighted_reward, next_state, done, weights))\n",
    "\n",
    "    def learn(self, step_count):\n",
    "        if len(self.memory) < self.batch_size or step_count % self.learn_every != 0:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, weights = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.FloatTensor(np.array(actions)).to(self.device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(self.device)\n",
    "        weights = torch.FloatTensor(np.array(weights)).to(self.device)\n",
    "        \n",
    "        # Critic 更新\n",
    "        next_actions = self.target_actor(next_states, weights)\n",
    "        next_q_values = self.target_critic(next_states, next_actions, weights)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        q_values = self.critic(states, actions, weights)\n",
    "        critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # 保存 Critic 损失\n",
    "        self.critic_loss_history.append(critic_loss.item())\n",
    "        \n",
    "        # Actor 更新\n",
    "        actions_pred = self.actor(states, weights)\n",
    "        actor_loss = -self.critic(states, actions_pred, weights).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        self._update_target(self.target_actor, self.actor, self.tau)\n",
    "        self._update_target(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "# 设备设置：使用 GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = FactoryEnv(state_size=73, action_size=73, B=B, max_steps=50000)\n",
    "agent = MORLAgent(state_size=73, action_size=73, shared_size=len(shared_indices), weight_size=3, device=device)\n",
    "\n",
    "# 记录训练数据\n",
    "reward_history = []\n",
    "critic_loss_history = []\n",
    "\n",
    "# 开始训练\n",
    "episodes = 100\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    weights = np.random.dirichlet(np.ones(3), size=1)[0]\n",
    "    step_count = 0\n",
    "\n",
    "    # 使用 tqdm 显示训练进度\n",
    "    with tqdm(total=env.max_steps, desc=f\"Episode {episode + 1}\") as pbar:\n",
    "        while not done:\n",
    "            action = agent.act(state, weights)\n",
    "            next_state, reward_vector, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward_vector, next_state, done, weights)\n",
    "            agent.learn(step_count)\n",
    "            state = next_state\n",
    "            episode_reward += np.dot(weights, reward_vector)\n",
    "            step_count += 1\n",
    "            pbar.update(1)  # 更新进度条\n",
    "\n",
    "    reward_history.append(episode_reward)\n",
    "    critic_loss_history.append(np.mean(agent.critic_loss_history[-step_count:]))  # 记录每个 episode 的平均 Critic 损失\n",
    "    print(f\"Episode {episode + 1}, Weighted Reward: {episode_reward}, Critic Loss: {critic_loss_history[-1]}\")\n",
    "\n",
    "# 绘制训练过程中的加权奖励和 Critic 损失图像\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(reward_history, label='Weighted Reward')\n",
    "plt.title('Weighted Reward Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Weighted Reward')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(critic_loss_history, label='Critic Loss')\n",
    "plt.title('Critic Loss Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Critic Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
